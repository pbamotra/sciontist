---
layout: post
type: both
tag: paper
title: Large-Scale Machine Learning with SGD
_url:
  - title: Zero Knowledge Proofs â€” A Primer
    type: fa-external-link
    uri: https://jeremykun.com/2016/07/05/zero-knowledge-proofs-a-primer/
    heart: <span style="color:#19B5FE" title="Fascinating stuff"><i class="fa fa-bolt" aria-hidden="true"></i></span>
  - title: Central Limit Theorem - It's sexy, so know it!
    type: fa-external-link
    uri: http://www.jeannicholashould.com/the-theorem-every-data-scientist-should-know.html
---
Link to paper: [LÂ´eon Bottou, Large-Scale Machine Learning
with Stochastic Gradient Descent](http://leon.bottou.org/publications/pdf/compstat-2010.pdf) <i class="fa fa-file-pdf-o" aria-hidden="true"></i>
<br />
<br />
Keypoints: -
- Under sufficient regularity assumptions, when the initial estimate w<sub>0</sub> is close enough to the optimum, and when the gain is sufficiently small, this algorithm achieves linear convergence , that is, ô€€€-log $\rho$ t, where   $\rho$ represents the residual error.
